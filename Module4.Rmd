---
title: "Module 4 Report"
author: '450132759 / 450463055'
date: "October 31, 2018"
output: 
  prettydoc::html_pretty:
    theme: cayman # prettydoc theme
    highlight: null # syntax highlighting
    css: custom.css # custom css file to change font family and size
---
<!-- <style> -->
<!-- @import url('https://fonts.googleapis.com/css?family=Roboto+Mono'); -->
<!-- @import url('https://fonts.googleapis.com/css?family=Lato'); -->
<!-- body{ -->
<!--   font-family: 'Lato' !important; -->
<!--   font-size: 12pt; -->
<!-- } -->

<!-- code{ -->
<!--   font-family: 'Roboto Mono' !important; -->
<!--   font-size: 12px; -->
<!-- } -->

<!-- pre{ -->
<!--   font-family: 'Roboto Mono' !important; -->
<!--   font-size: 12px -->
<!-- } -->

<!-- td{ -->
<!--   font-family: Lato !important; -->
<!--   font-size: 12pt; -->
<!-- } -->
<!-- </style> -->

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, fig.align = "center")
```

##Executive Summary
In this report, we use various machine learning algorithms to make a prediction on whether an email is flagged as spam or nonspam. It is found that a binomial logistic model gives the best accuracy, followed by a decision tree model, and then a k-nearest neighbour model.
##Introduction
Flagging spam email for users is a crucial part of any email service, as it offers protection for users from phishing scams or from nefarious companies from flooding a user's inbox and drowing out legitimate emails. Fortunately, spam emails frequently contain certain characters or phrases that can easily be identified by scanning incoming emails. We obtain the data set from the University of Irvine Machine Learning Repository, which contains percentages of appearances of 56 different words or characters in an email. We also know that each of the 4601 emails in our data set is flagged as either spam or nonspam. We will start by fitting different models to our dataset and then identify which model gives the best accuracy. It should be noted that classifying a spam email as non-spam is a less serious error as misclassifying a clean email as spam, so this will also be considered in the following analysis.
##Analysis
```{r import}
library(tidyverse)
library(gridExtra)
library(emmeans)
library(ggfortify)
# install.packages('kernlab')
data(spam, package = "kernlab")
```

```{r cleaning}
spam <- spam %>% mutate(type = ifelse(type == "spam", 1, 0))
spam$type = as.factor(spam$type)
```

###Decision Tree/Random Forest
```{r}
#install.packages("rpart")
#install.packages("rpart.plot")
library(rpart)
library(rpart.plot)
tree = rpart(factor(type) ~ ., data = spam, method = "class")
rpart.plot(tree)
```

```{r in.sample}
library(caret)
predicted = predict(tree, type = "class")
confusionMatrix(
  data = predicted,
  reference = factor(spam$type),
  positive = "1")
```
In sample performance of decision tree model gives an accuracy of 90.26%. We also observe a specificity value of 0.9519 and a sensitivity value of 0.8268. The high specificity value is especially desirable in this case, as it indicates a low falso positive rate. This means that clean emails are rarely classified as spam.

```{r complexity}
train(type ~ ., data = spam,
     method = "rpart", trControl = trainControl(method = "cv", number = 10))
```
Above, we look at finding the complexity parameter that gives the least RMSE, which is cp = 0.079.

```{r randomforest}
library(randomForest)
tree_rf = randomForest(type ~ ., spam)
tree_rf
```


###Logistic Regression
```{r logistic_regression, cache=TRUE, results='hide'}
# install.packages("stargazer")
# library(stargazer)
spam_full_glm <- glm(type ~ ., family = binomial, data = spam)
spam_bw_aic = step(spam_full_glm,direction = "backward",trace=TRUE)
spam_null_glm <- glm(type ~ 1, family = binomial, data = spam)
spam_fw_aic = step(spam_full_glm,direction = "forward",trace=TRUE)
```

```{r log_regree_performance}

spam = spam %>% mutate(pred_bw = predict(spam_bw_aic, type = "response"),
         pred_bw = round(pred_bw))

mean(spam$type != spam$pred_bw)
library(caret)
confusion.glm = confusionMatrix(
  data = as.factor(spam$pred_bw), 
  reference = as.factor(spam$type))
confusion.glm
# stargazer::stargazer(fm, step_model, type = "html", column.labels = c("Full model",
    # "Stepwise model"))

spam = spam %>% mutate(pred_fw = predict(spam_fw_aic, type = "response"),
         pred_fw = round(pred_fw))

mean(spam$type != spam$pred_fw)
library(caret)
confusion.glm = confusionMatrix(
  data = as.factor(spam$pred_fw), 
  reference = as.factor(spam$type))
confusion.glm

# train(spam_bw_aic,
#       data = spam, 
#       method = "glm",
#       family = "binomial",
#       trControl = trainControl(
#         method = "cv", number = 5,
#         verboseIter = FALSE
#       ))
```
We observe a 93% accuracy from the backwards AIC logistic model.
###Nearest Neighbour
```{r in.sample.5, cache = TRUE}
knn5 = class::knn(train = spam, test = spam, cl = spam$type, k = 5)
caret::confusionMatrix(knn5, spam$type)$table
caret::confusionMatrix(knn5, spam$type)$overall[1] %>% round(2)
```

```{r out.of.sample, cache = TRUE}
fitControl = trainControl(method = "repeatedcv", number = 5, repeats = 10)
knnFit1 = train(type ~ ., data = spam, method = "knn", trControl = fitControl, 
    tuneLength = 10)
knnFit1
```

```{r}
# knn_acc = max(knnFit1$results$Accuracy)
# # decision tree
# rpartFit1 = train(type ~ ., data = spam, method = "rpart", trControl = fitControl)
# rpart_acc = max(rpartFit1$results$Accuracy)
# # random forests
# rfFit1 = train(type ~ ., data = spam, method = "rf", trControl = fitControl)
# rf_acc = max(rfFit1$results$Accuracy)
# # glm
# glmFit1 = train(type ~ ., data = spam, method = "glm", family = "binomial", 
#     trControl = fitControl)
# glm_acc = glmFit1$results$Accuracy
# knn_acc
```



##Conclusion
