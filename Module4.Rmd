---
title: "Module 4 Report"
author: '450132759 / 450463055'
date: "October 31, 2018"
output: 
  prettydoc::html_pretty:
    theme: cayman # prettydoc theme
    highlight: null # syntax highlighting
    css: custom.css # custom css file to change font family and size
---
<!-- <style> -->
<!-- @import url('https://fonts.googleapis.com/css?family=Roboto+Mono'); -->
<!-- @import url('https://fonts.googleapis.com/css?family=Lato'); -->
<!-- body{ -->
<!--   font-family: 'Lato' !important; -->
<!--   font-size: 12pt; -->
<!-- } -->

<!-- code{ -->
<!--   font-family: 'Roboto Mono' !important; -->
<!--   font-size: 12px; -->
<!-- } -->

<!-- pre{ -->
<!--   font-family: 'Roboto Mono' !important; -->
<!--   font-size: 12px -->
<!-- } -->

<!-- td{ -->
<!--   font-family: Lato !important; -->
<!--   font-size: 12pt; -->
<!-- } -->
<!-- </style> -->

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, fig.align = "center")
```

## Executive Summary
In this report, we use various machine learning algorithms to make a prediction on whether an email is flagged as spam or nonspam. It is found that a binomial logistic model gives the best accuracy, followed by a decision tree model, and then a k-nearest neighbour model.

## Introduction
Flagging spam email for users is a crucial part of any email service, as it offers protection for users from phishing scams or from nefarious companies from flooding a user's inbox and drowing out legitimate emails. Fortunately, spam emails frequently contain certain characters or phrases that can easily be identified by scanning incoming emails. We obtain the data set from the University of Irvine Machine Learning Repository, which contains percentages of appearances of 56 different words or characters in an email. We also know that each of the 4601 emails in our data set is flagged as either spam or nonspam. As we do not want to misclassify legitimate emails as spam emails, our performance metric is the specificity of our models. We will start by fitting different models to our dataset and then identify which model gives the highest specificity.

## Analysis

```{r import}
library(tidyverse)
library(gridExtra)
library(emmeans)
library(ggfortify)
# install.packages('kernlab')
data(spam, package = "kernlab")
```

```{r cleaning}
spam <- spam %>% mutate(type = ifelse(type == "spam", 1, 0))
spam$type = as.factor(spam$type)
```
Here, we changed the spam and nonspam labels to 1 and 0 respectively, and specified them as factors.


### Decision Tree
```{r}
#install.packages("rpart")
#install.packages("rpart.plot")
library(rpart)
library(rpart.plot)
tree = rpart(factor(type) ~ ., data = spam, method = "class")
rpart.plot(tree)
```

```{r in.sample}
library(caret)
predicted = predict(tree, type = "class")
cm = confusionMatrix(
  data = predicted,
  reference = factor(spam$type),
  positive = "1")
cm$overall[1] %>% round(2)
cm$byClass[2]
```
In sample performance of decision tree model gives an accuracy of 90.26%. We also observe a specificity value of 0.9519 and a sensitivity value of 0.8268. The high specificity value is especially desirable in this case, as it indicates a low false positive rate. This means that clean emails are rarely classified as spam.

```{r complexity}
train(type ~ ., data = spam,
     method = "rpart", trControl = trainControl(method = "cv", number = 10))
```
Above, we look at finding the complexity parameter that gives the least RMSE, which is cp = 0.0430, around 4.3%. The complexity parameter gives an out of sample accuracy of about 85.7%.

```{r}
tree_cp = rpart(factor(type) ~ ., data = spam, method = "class", control = rpart.control(cp = 0.043))
rpart.plot(tree)
```

```{r}
library(caret)
predicted_cp = predict(tree_cp, type = "class")
confusionMatrix(
  data = predicted_cp,
  reference = factor(spam$type),
  positive = "1")
```
While this model suffers a drop of about 2% in both accuracy and specificity, the improved out-of-sample performance is preferred, as it allows the spam detection to deal with a wider range of emails, making it a more robust system.

### Random Forest

Now, we look to fit a random forest model. 
```{r randomforest, cache=TRUE}
library(randomForest)
tree_rf = randomForest(type ~ ., spam)
tree_rf
predicted_cp = predict(tree_rf, type = "class")
confusionMatrix(
  data = predicted_cp,
  reference = factor(spam$type),
  positive = "1")$byClass[2]
```
We observe an out-of-bag estimated error rate of about 4.5%, which translate to an accuracy of about 95.5%. This is better than the observed accuracy from the decision tree model. Additionally the class error rate for the clean emails is even lower than the overall estimated error, resulting in a low rate of incorrectly labelling clean emails as spam. We also obtain a specificity value of 96.7%.

### Logistic Regression
Here, we seek to fit a binomial logistic model. We find a suitable model using backwards and forwards AIC.

```{r logistic_regression, cache=TRUE, results='hide'}
# install.packages("stargazer")
# library(stargazer)
spam_full_glm <- glm(type ~ ., family = binomial, data = spam)
spam_bw_aic = step(spam_full_glm,direction = "backward",trace=TRUE)
spam_null_glm <- glm(type ~ 1, family = binomial, data = spam)
spam_fw_aic = step(spam_full_glm,direction = "forward",trace=TRUE)
```

```{r log_regree_performance}

spam = spam %>% mutate(pred_bw = predict(spam_bw_aic, type = "response"),
         pred_bw = round(pred_bw))

mean(spam$type != spam$pred_bw)
library(caret)
confusion.glm = confusionMatrix(
  data = as.factor(spam$pred_bw), 
  reference = as.factor(spam$type),
  positive = "1")
confusion.glm$overall[1]
confusion.glm$byClass[2]
# stargazer::stargazer(fm, step_model, type = "html", column.labels = c("Full model",
    # "Stepwise model"))

spam = spam %>% mutate(pred_fw = predict(spam_fw_aic, type = "response"),
         pred_fw = round(pred_fw))

mean(spam$type != spam$pred_fw)
library(caret)
confusion.glm = confusionMatrix(
  data = as.factor(spam$pred_fw), 
  reference = as.factor(spam$type),
  positive = "1")
confusion.glm$overall[1]
confusion.glm$byClass[2]
```
We observe a 93.22% accuracy from the backwards AIC logistic model, and a 93.13% accuracy from the forwards AIC logistic model. Furthermore, we observed specificity values of 95.73% and 95.62%, respectively. 

The backwards AIC model has 37 of the original 56 variables, and they are shown below:
```{r}
spam_bw_aic$model %>% names()
```


### Nearest Neighbour

Here, we look at fitting a nearest neighbour model with k = 5.
```{r in.sample.5, cache = TRUE}
knn5 = class::knn(train = spam, test = spam, cl = spam$type, k = 5)
confusionMatrix(knn5, spam$type, positive = "1")$byClass[2]
caret::confusionMatrix(knn5, spam$type)$table
caret::confusionMatrix(knn5, spam$type)$overall[1] %>% round(2)
```
Here, we observe a specificity of 91.07%, with an accuracy of 88.5%. This model performs worse than the decision tree, random forest, or logistic regression model.

We now check that k = 5 is the value of k that gives the best accuracy by using repeated cross-validation.
```{r out.of.sample, cache = TRUE}
fitControl = trainControl(method = "repeatedcv", number = 5, repeats = 10)
knnFit1 = train(type ~ ., data = spam, method = "knn", trControl = fitControl, 
    tuneLength = 10)
knnFit1
```


Here, we look at the out of sample performance for kNN, decision trees, and logistic regression model.
```{r overall, cache = TRUE}
knn_acc = max(knnFit1$results$Accuracy)
# decision tree
rpartFit1 = train(type ~ ., data = spam, method = "rpart", trControl = fitControl)
rpart_acc = max(rpartFit1$results$Accuracy)
# random forests
# rfFit1 = train(type ~ ., data = spam, method = "rf", trControl = trainControl(method = "repeatedcv", number = 5, repeats = 1))
# rf_acc = max(rfFit1$results$Accuracy)
# rf_acc
# # glm
glmFit1 = train(type ~ ., data = spam, method = "glm", family = "binomial",
    trControl = fitControl)
glm_acc = glmFit1$results$Accuracy
c(knn_acc,rpart_acc,glm_acc)
```

We can see that the decision tree model gives the best accuracy, followed by the logistic regression model and the k-nearest neighbour model.

##Conclusion
